"""Train a tiny U-Net denoiser for kymograph trajectories.

The goal is to learn a mapping noisy_kymograph -> clean_kymograph using
synthetic data generated by helpers.generate_kymograph. The network is a small
2D U-Net operating on (time, position) images. Training can finish within a few
minutes on CPU for the default settings.

Training dimensions:
- length (time): 512 (can be chunked for longer sequences)
- width (position): 512 (must match validation data dimension)

Validation data is 20000x512 (time x position). The model processes longer
sequences by chunking along the time dimension.
"""

from dataclasses import dataclass
from typing import Tuple

import numpy as np
import torch
from torch import nn, optim
from torch.utils.data import Dataset, DataLoader

from helpers import generate_kymograph, get_diffusion_coefficient


class SyntheticKymographDataset(Dataset):
    """On-the-fly dataset of synthetic noisy/clean kymographs."""

    def __init__(
        self,
        length: int = 512,  # Time dimension (can be chunked)
        width: int = 512,  # Position dimension (must match validation: 512)
        radii_nm: Tuple[float, float] = (3.0, 15.0),
        contrast: Tuple[float, float] = (0.5, 1.0),
        noise_level: Tuple[float, float] = (0.1, 0.5),
        seed: int | None = None,
        peak_width: float = 1.0,
        dt: float = 1.0,
        dx: float = 0.5,
        n_samples: int = 1024,
    ) -> None:
        self.length = length
        self.width = width
        self.radii_nm = radii_nm
        self.contrast = contrast
        self.noise_level = noise_level
        self.peak_width = peak_width
        self.dt = dt
        self.dx = dx
        self.n_samples = n_samples
        self.rng = np.random.default_rng(seed)

    def __len__(self) -> int:  # type: ignore[override]
        return self.n_samples

    def sample_parameters(self) -> Tuple[float, float, float]:
        radius = float(self.rng.uniform(*self.radii_nm))
        contrast = float(self.rng.uniform(*self.contrast))
        noise = float(self.rng.uniform(*self.noise_level))
        return radius, contrast, noise

    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:  # type: ignore[override]
        radius, contrast, noise = self.sample_parameters()
        diffusion = get_diffusion_coefficient(radius)
        noisy, gt, _ = generate_kymograph(
            length=self.length,
            width=self.width,
            diffusion=diffusion,
            contrast=contrast,
            noise_level=noise,
            peak_width=self.peak_width,
            dt=self.dt,
            dx=self.dx,
        )
        noisy_tensor = torch.from_numpy(noisy).unsqueeze(0).float()
        gt_tensor = torch.from_numpy(gt).unsqueeze(0).float()
        return noisy_tensor, gt_tensor


class ConvBlock(nn.Module):
    def __init__(self, in_ch: int, out_ch: int) -> None:
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore[override]
        return self.net(x)


class TinyUNet(nn.Module):
    """Minimal U-Net with three resolution levels."""

    def __init__(self, base_channels: int = 32) -> None:
        super().__init__()
        self.enc1 = ConvBlock(1, base_channels)
        self.enc2 = ConvBlock(base_channels, base_channels * 2)
        self.enc3 = ConvBlock(base_channels * 2, base_channels * 4)

        self.down = nn.MaxPool2d(2)

        self.bottleneck = ConvBlock(base_channels * 4, base_channels * 8)

        self.up3 = nn.ConvTranspose2d(base_channels * 8, base_channels * 4, 2, stride=2)
        self.dec3 = ConvBlock(base_channels * 8, base_channels * 4)

        self.up2 = nn.ConvTranspose2d(base_channels * 4, base_channels * 2, 2, stride=2)
        self.dec2 = ConvBlock(base_channels * 4, base_channels * 2)

        self.up1 = nn.ConvTranspose2d(base_channels * 2, base_channels, 2, stride=2)
        self.dec1 = ConvBlock(base_channels * 2, base_channels)

        self.out_conv = nn.Conv2d(base_channels, 1, kernel_size=1)

    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore[override]
        e1 = self.enc1(x)
        e2 = self.enc2(self.down(e1))
        e3 = self.enc3(self.down(e2))

        b = self.bottleneck(self.down(e3))

        d3 = self.up3(b)
        d3 = torch.cat([d3, e3], dim=1)
        d3 = self.dec3(d3)

        d2 = self.up2(d3)
        d2 = torch.cat([d2, e2], dim=1)
        d2 = self.dec2(d2)

        d1 = self.up1(d2)
        d1 = torch.cat([d1, e1], dim=1)
        d1 = self.dec1(d1)

        return torch.sigmoid(self.out_conv(d1))


def _default_device() -> str:
    has_mps = getattr(torch.backends, "mps", None)
    if has_mps and torch.backends.mps.is_available():
        return "mps"
    if torch.cuda.is_available():
        return "cuda"
    return "cpu"


@dataclass
class TrainingConfig:
    epochs: int = 5
    batch_size: int = 8
    lr: float = 1e-3
    loss: str = "l2"  # "l2" or "bce"
    device: str = _default_device()


def get_loss_fn(name: str) -> nn.Module:
    name = name.lower()
    if name == "l2":
        return nn.MSELoss()
    if name == "bce":
        return nn.BCELoss()
    raise ValueError(f"Unsupported loss '{name}'")


def train_denoiser(config: TrainingConfig, dataset: SyntheticKymographDataset) -> TinyUNet:
    model = TinyUNet().to(config.device)
    dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True)
    optimizer = optim.Adam(model.parameters(), lr=config.lr)
    criterion = get_loss_fn(config.loss)

    model.train()
    for epoch in range(config.epochs):
        epoch_loss = 0.0
        for noisy, clean in dataloader:
            noisy = noisy.to(config.device)
            clean = clean.to(config.device)
            optimizer.zero_grad()
            denoised = model(noisy)
            loss = criterion(denoised, clean)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item() * noisy.size(0)
        epoch_loss /= len(dataloader.dataset)
        print(f"Epoch {epoch + 1}/{config.epochs}: loss={epoch_loss:.4f}")

    return model


def save_model(model: nn.Module, path: str) -> None:
    torch.save(model.state_dict(), path)


def load_model(path: str, device: str | None = None) -> TinyUNet:
    device = device or ("cuda" if torch.cuda.is_available() else "cpu")
    model = TinyUNet().to(device)
    state = torch.load(path, map_location=device)
    model.load_state_dict(state)
    model.eval()
    return model


def denoise_kymograph(model: TinyUNet, kymograph: np.ndarray, device: str | None = None) -> np.ndarray:
    device = device or ("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    model.eval()
    with torch.no_grad():
        tensor = torch.from_numpy(kymograph).unsqueeze(0).unsqueeze(0).float().to(device)
        denoised = model(tensor)
        return denoised.squeeze(0).squeeze(0).cpu().numpy()


if __name__ == "__main__":
    # Training dimensions: length=512 (time, chunkable), width=512 (position, matches validation)
    dataset = SyntheticKymographDataset(n_samples=1024, length=512, width=512)
    config = TrainingConfig(epochs=5, batch_size=8, loss="l2")
    model = train_denoiser(config, dataset)
    save_model(model, "tiny_unet_denoiser.pth")
